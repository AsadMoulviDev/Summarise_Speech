{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarise_Speech.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSwnswk9Wrf8PQM4XwVLY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsadMoulviDev/Summarise_Speech/blob/main/Summarise_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsFRjFZlX3sw"
      },
      "source": [
        "pip install transformers \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD9-FLpx2Igz"
      },
      "source": [
        "pip install rpunct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarise Speech \n",
        "The aim of this project was to generate a text summarty of President Bidens speech.\n",
        "\n",
        "Step 1: To convert the full speech to text. \n",
        "\n",
        "Step 2: To summarize the speech in a few lines \n"
      ],
      "metadata": {
        "id": "eHgLW9bwe1_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Functions"
      ],
      "metadata": {
        "id": "6OYt5SBVjnyI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xljFJDei7O6g"
      },
      "source": [
        "def split_fun(num):\n",
        "\n",
        "  for i in range(1,num+1):\n",
        "    if num%i==0:\n",
        "      if num/i <150000:\n",
        "        return i \n",
        "\n",
        "def summarize(text, max_length=200):\n",
        "  input_ids = tokenizer_sum.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "  generated_ids = model_sum.generate(input_ids=input_ids, num_beams=2, max_length=max_length,  repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "\n",
        "  preds = [tokenizer_sum.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "\n",
        "  return preds[0]\n",
        "\n",
        "def split_padded(a,n):\n",
        "  padding = (-len(a))%n\n",
        "  return np.split(np.concatenate((a,np.zeros(padding))),n)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Transfomer Models\n",
        "Taking open source pre trained model such as facebooks wav2vec model to tokenize the audio files."
      ],
      "metadata": {
        "id": "QrqZW6kKkG_E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LLlPOWMXNis"
      },
      "source": [
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "from rpunct import RestorePuncts\n",
        "import numpy as np\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "rpunct = RestorePuncts()\n",
        "tokenizer_sum = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
        "model_sum = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse and generate text from the audio file\n"
      ],
      "metadata": {
        "id": "w1HN3TYuk0ii"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q2zdz8ahNgG",
        "outputId": "3072622b-d3f2-4f03-f739-0a62e5d84e06"
      },
      "source": [
        "\n",
        "speech, rate = librosa.load(\"/content/16f91154-5db0-4d43-984d-199cf7c3c6d3.wav\",sr = 16000)\n",
        "print(len(speech))\n",
        "sp1 =split_padded(speech,25)\n",
        "print(len(sp1[1]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4081586\n",
            "163264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETuZmcOihX8O"
      },
      "source": [
        "text = []\n",
        "for i in sp1:\n",
        "  input_values = tokenizer(i, return_tensors = 'pt').input_values\n",
        "  logits = model(input_values).logits\n",
        "  predicted_ids = torch.argmax(logits, dim =-1)\n",
        "  transcriptions = tokenizer.decode(predicted_ids[0])\n",
        "  text.append(transcriptions.lower())\n",
        "listToStr = ' '.join([str(elem) for elem in text])\n",
        "Full_text = rpunct.punctuate(listToStr)\n",
        "#2h 16m 12s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "gfX-KFA89xTA",
        "outputId": "56b81083-9ffc-46f5-a541-ab3b2b515a38"
      },
      "source": [
        "Full_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"We've had an enormous expansion of inequality. I think in many ways our perception of the society we live in has not at all caught up with the reality, which is fully a return to Gilded Age levels in the United States. actually tomapicts. A nice point was the Gilded Age was a lot of self made men and what we're having increasingly looks instead more like his own country's Belle Apoc, which was largely a society dominated by inherited wealth. And that that's a very important thing to note. Let me give you three itions that are often adduced about the role of higher education. and all this a thing is, you don't have to believe in all of them. In fact, you shouldn't believe in two of them. The other hand, the third one is very very much true. The first proposition is that O education skills are at the core of what's been happening. Are the the story about why inequality is widening so much? The reason that we're having trouble creating jobs is because workers don't have the right skills. That the reason we're seeing widening inequality is that only some workers have the skill to participate in this modern high technology economy. The second proposition is that highly educated workers are big winners in story. The third proposition is that access to higher education e were the rungs on the ladder are available, particularly for those who who start out from a lower socio economic status is more crucial than ever. So that last proposition is true. The first two are things that people think are probably not true. The idea that that that what we have right now is a skills gap. Ah and tat. That's what's holding the economy back. Unemployment rates among the college educated are lower than they are and the population at large. but that's always true. They're far higher than they were Before the Financial crisis, this has been a really equal opportunity recession. It has ed almost everybody. If you're looking for groups that have done well in the economic crisis, particular skill categories, they're very hard to find. Almost nobody's wages a going up significantly. The exceptions are funny. I mean, not funny. they are. Theyare important that the people get them, but they tend to be, if anything, a manual skills that happen to be in short supply. Welders are doing quite well. Right now. There's been a surge in wages for people who know how to operate sewing machines because there's been some reshoring of the apparel industry, but there was a wide hypothesis just taking a longer term view. the hypothesis of skilled bias, technological change it's so common in in these discussions, people just say S B T C and you're supposed to know what that is. A view was that what we had with technology was bi biasing the economy towards needing evermore in the way of highly skilled andi particularly high formal education workers, and that that was the explanation of rising a inequality a fifteen years ago. It was very widely accepted among labor economists at this point, it's largely fallen apart. The evidence just doesn't seem support that story. A doesn't support it in a couple of ways. One is that a large part of the increase in inequality is actually an increase in inequality among the highly educated. As I like to sayhi, fund managers and high school teachers have similar level of education. Ah, if you look at the details, it just doesn't seem to work. And also excess in wages of college graduates over non graduates. Wele, it did widen a lot between about Nineteen Eighty and Two Thousand hasn't widened much since then. While inequality has, of course continued to rise. The the story that identifies inequal with a premium to education is fifteen years out of date. It has not really been sustainable as a story of what's happening now. A As to the highly educated being big winners, Well, certainly better to have that good educationad the advantage of the highly educated over the those without is a lot bigger than it was in Nineteen Eighty. In that sense, it is very important to have that education. But it turns out that at the meeti the medium college educated worker is seen decline in wages. Although the U S economy, despite the crisis, is a lot richer than it was fifteen years ago. Ah, none of that has trickled down, even to college graduates.\""
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize the text\n"
      ],
      "metadata": {
        "id": "dY6Ng83rldAl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "U_pJYy5TdY8s",
        "outputId": "3ba7ce55-3163-4e68-f2de-53a753af8bf2"
      },
      "source": [
        "summarize(Full_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I want to thank everyone who voted to support the Billback Better Agenda. As of today, seventy thousand, Seven hundred people have been evacuated in Afghanistan and fifty more flights since T Two hours. This is a testament to the brave service women and men who served alongside their forces as translators or in their embassies. The bill also calls for strengthening our mutual obligation to support refugee and evacuates currently flay in Afghanistan. Notably, this is the fastest economic growth rate in forty years.nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQVkT-cdlvK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}